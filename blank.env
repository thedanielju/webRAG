# ╔══════════════════════════════════════════════════════════════╗
# ║                     WebRAG Configuration                     ║
# ║    Copy this file to .env and fill required values.          ║       
# ║  Lines starting with # are comments and are ignored.         ║
# ╚══════════════════════════════════════════════════════════════╝
# ------------------------------------------------------------
# Firecrawl
# ------------------------------------------------------------
# Get your API key at https://firecrawl.dev
FIRECRAWL_API_KEY=fc-

# Max links returned when mapping a site (default: 100)
# FIRECRAWL_MAP_DEFAULT_LIMIT=100

# Max links to discover when crawling (default: 500)
# INGEST_DISCOVER_LINKS_DEFAULT_LIMIT=500


# ------------------------------------------------------------
# Database (Postgres + pgvector)
# ------------------------------------------------------------
# Use plain psycopg DSN format for psycopg.connect
# Default matches docker-compose.yml
DATABASE_URL=postgresql://webrag:webrag@localhost:5432/webrag


# ── Embedding ────────────────────────────────────────────────────
# Default: OpenAI text-embedding-3-small (recommended, low cost)
# To use a local model instead, see the Local Embedding section below.

EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_API_KEY=sk-...
EMBEDDING_DIMENSIONS=1536
EMBEDDING_TOKENIZER_KIND=tiktoken
EMBEDDING_TOKENIZER_NAME=cl100k_base

# ── Local Embedding (optional alternative to OpenAI) ─────────────
# Uncomment and fill in to use a local model via Ollama or LM Studio.
# You must also comment out the OpenAI embedding fields above.
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_API_KEY=
# EMBEDDING_DIMENSIONS=768
# EMBEDDING_TOKENIZER_KIND=huggingface
# EMBEDDING_TOKENIZER_NAME=nomic-ai/nomic-embed-text-v1


# ── Embedding Concurrency ────────────────────────────────────────
# Number of texts per embedding API request (default: 256).
# Larger batches reduce HTTP overhead; smaller batches lower per-request
# latency and memory.  256 is a good balance for OpenAI and local servers.
# EMBEDDING_BATCH_SIZE=256

# Max parallel threads sending embedding batches (default: 4).
# Keep this conservative for OpenAI to avoid 429 rate-limit errors on
# lower API tiers.  Paid plans with higher RPM can safely use 8-12.
# Local servers are not rate-limited - feel free to increase
# as desired up to CPU/GPU core count.
# EMBEDDING_MAX_WORKERS=4


# ── Chunking ─────────────────────────────────────────────────────
# Target token size for child chunks (default: 256)
# CHILD_TARGET_TOKENS=256

# Maximum token size for parent chunks before splitting (default: 1000)
# PARENT_MAX_TOKENS=1000


# ------------------------------------------------------------
# Retrieval
# ------------------------------------------------------------
# Corpus token threshold for full-context vs chunk mode (default: 30000)
# Must be <= RETRIEVAL_CONTEXT_BUDGET (retrieval clamps if higher).
# RETRIEVAL_FULL_CONTEXT_THRESHOLD=30000

# Max tokens of retrieved context to return (default: 40000)
# Must be >= RETRIEVAL_FULL_CONTEXT_THRESHOLD.
# RETRIEVAL_CONTEXT_BUDGET=40000

# Max child chunks from HNSW search before parent aggregation (default: 60)
# RETRIEVAL_TOP_K_CHILDREN_LIMIT=60

# Cosine similarity floor -- children below this are discarded (default: 0.3)
# RETRIEVAL_SIMILARITY_FLOOR=0.3

# Depth decay rate per level (default: 0.05)
# RETRIEVAL_DEPTH_DECAY_RATE=0.05

# Minimum depth multiplier floor (default: 0.80)
# RETRIEVAL_DEPTH_FLOOR=0.80

# HNSW ef_search recall setting (default: 100)
# RETRIEVAL_HNSW_EF_SEARCH=100
