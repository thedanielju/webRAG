# ╔══════════════════════════════════════════════════════════════╗
# ║                     WebRAG Configuration                     ║
# ║    Copy this file to .env and fill required values.          ║       
# ║  Lines starting with # are comments and are ignored.         ║
# ╚══════════════════════════════════════════════════════════════╝
# ------------------------------------------------------------
# Firecrawl
# ------------------------------------------------------------
# Get your API key at https://firecrawl.dev
FIRECRAWL_API_KEY=fc-

# Max links returned when mapping a site (default: 100)
# FIRECRAWL_MAP_DEFAULT_LIMIT=100

# Max links to discover when crawling (default: 500)
# INGEST_DISCOVER_LINKS_DEFAULT_LIMIT=500


# ------------------------------------------------------------
# Database (Postgres + pgvector)
# ------------------------------------------------------------
# Use plain psycopg DSN format for psycopg.connect
# Default matches docker-compose.yml
DATABASE_URL=postgresql://webrag:webrag@localhost:5432/webrag


# ── Embedding ────────────────────────────────────────────────────
# Default: OpenAI text-embedding-3-small (recommended, low cost)
# To use a local model instead, see the Local Embedding section below.

EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_API_KEY=sk-...
EMBEDDING_DIMENSIONS=1536
EMBEDDING_TOKENIZER_KIND=tiktoken
EMBEDDING_TOKENIZER_NAME=cl100k_base

# ── Local Embedding (optional alternative to OpenAI) ─────────────
# Uncomment and fill in to use a local model via Ollama or LM Studio.
# You must also comment out the OpenAI embedding fields above.
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_API_KEY=
# EMBEDDING_DIMENSIONS=768
# EMBEDDING_TOKENIZER_KIND=huggingface
# EMBEDDING_TOKENIZER_NAME=nomic-ai/nomic-embed-text-v1


# ── Embedding Concurrency ────────────────────────────────────────
# Number of texts per embedding API request (default: 256).
# Larger batches reduce HTTP overhead; smaller batches lower per-request
# latency and memory.  256 is a good balance for OpenAI and local servers.
# EMBEDDING_BATCH_SIZE=256

# Max parallel threads sending embedding batches (default: 4).
# Keep this conservative for OpenAI to avoid 429 rate-limit errors on
# lower API tiers.  Paid plans with higher RPM can safely use 8-12.
# Local servers are not rate-limited - feel free to increase
# as desired up to CPU/GPU core count.
# EMBEDDING_MAX_WORKERS=4


# ── Chunking ─────────────────────────────────────────────────────
# Target token size for child chunks (default: 256)
# CHILD_TARGET_TOKENS=256

# Maximum token size for parent chunks before splitting (default: 1000)
# PARENT_MAX_TOKENS=1000


# ------------------------------------------------------------
# Retrieval
# ------------------------------------------------------------
# Corpus token threshold for full-context vs chunk mode (default: 30000)
# Must be <= RETRIEVAL_CONTEXT_BUDGET (retrieval clamps if higher).
# RETRIEVAL_FULL_CONTEXT_THRESHOLD=30000

# Max tokens of retrieved context to return (default: 40000)
# Must be >= RETRIEVAL_FULL_CONTEXT_THRESHOLD.
# RETRIEVAL_CONTEXT_BUDGET=40000

# Max child chunks from HNSW search before parent aggregation (default: 60)
# RETRIEVAL_TOP_K_CHILDREN_LIMIT=60

# Cosine similarity floor -- children below this are discarded (default: 0.3)
# RETRIEVAL_SIMILARITY_FLOOR=0.3

# Depth decay rate per level (default: 0.05)
# RETRIEVAL_DEPTH_DECAY_RATE=0.05

# Minimum depth multiplier floor (default: 0.80)
# RETRIEVAL_DEPTH_FLOOR=0.80

# HNSW ef_search recall setting (default: 100)
# RETRIEVAL_HNSW_EF_SEARCH=100


# ------------------------------------------------------------
# Orchestration: Query Analysis
# ------------------------------------------------------------
# LLM for query decomposition (default: OpenAI gpt-4o-mini)
# ORCHESTRATION_LLM_BASE_URL=https://api.openai.com/v1
ORCHESTRATION_LLM_API_KEY=sk-...
# ORCHESTRATION_LLM_MODEL=gpt-4o-mini

# Decomposition mode: "llm" | "rule_based" | "none"
# - llm: Best quality, uses orchestration LLM. Falls back to rule_based
#   if ORCHESTRATION_LLM_API_KEY is not set.
# - rule_based: Regex/pattern-based. No API calls.
# - none: No decomposition, single query passed directly.
# DECOMPOSITION_MODE=llm

# ------------------------------------------------------------
# Orchestration: Reranking
# ------------------------------------------------------------
# Provider: "zeroentropy" | "cohere" | "jina" | "none"
# RERANKER_PROVIDER=zeroentropy

# Set RERANKER_API_KEY in .env for your chosen provider
RERANKER_API_KEY=

# Provider-specific model identifiers:
#   zeroentropy: "zerank-2" (default), "zerank-1"
#   cohere: "rerank-v3.5"
#   jina: "jina-reranker-v2-base-multilingual"
# RERANKER_MODEL=zerank-2

# Max results returned after reranking (default: 20)
# RERANKER_TOP_N=20

# ------------------------------------------------------------
# Orchestration: Expansion
# ------------------------------------------------------------
# Absolute safety ceiling for expansion iterations (default: 5)
# MAX_EXPANSION_DEPTH=5

# Top-scored candidates to scrape per expansion round (default: 5)
# MAX_CANDIDATES_PER_ITERATION=5

# Candidates to evaluate/score per iteration (default: 20)
# CANDIDATES_TO_SCORE_PER_ITERATION=20

# Limit passed to discover_links /map endpoint (default: 100)
# EXPANSION_MAP_LIMIT=100

# ------------------------------------------------------------
# Orchestration: Locality Expansion
# ------------------------------------------------------------
# Grab sibling parent chunks adjacent to high-scoring hits (default: true)
# LOCALITY_EXPANSION_ENABLED=true

# Number of sibling chunks in each direction (default: 1)
# LOCALITY_EXPANSION_RADIUS=1

# ------------------------------------------------------------
# Orchestration: Stopping Criteria
# ------------------------------------------------------------
# STOP if non-redundant tokens / budget > this ratio (default: 0.8)
# TOKEN_BUDGET_SATURATION_RATIO=0.8

# Pairwise cosine similarity ceiling for redundancy (default: 0.85)
# REDUNDANCY_CEILING=0.85

# Score gap between rank-1 and rank-K for cliff detection (default: 0.15)
# SCORE_CLIFF_THRESHOLD=0.15

# Which rank to compare against rank-1 (default: 5)
# SCORE_CLIFF_RANK_K=5

# Score variance threshold for plateau detection (default: 0.02)
# PLATEAU_VARIANCE_THRESHOLD=0.02

# Top N chunks for plateau/variance computation (default: 10)
# PLATEAU_TOP_N=10

# Min recall improvement to justify another expansion (default: 0.03)
# DIMINISHING_RETURN_DELTA=0.03

# Score below which a plateau is "mediocre" (default: 0.5)
# With raw embedding similarity (no reranker), lower to ~0.35.
# MEDIOCRE_SCORE_FLOOR=0.5

# ZeroEntropy min average confidence (default: 0.3)
# CONFIDENCE_FLOOR=0.3


# ------------------------------------------------------------
# MCP Server
# ------------------------------------------------------------
# Transport: "stdio" for desktop clients (Claude Desktop, Cursor) or
# "streamable-http" for remote/hosted deployments.
# MCP_TRANSPORT=stdio

# Network bind address and port for streamable-http (ignored for stdio).
# MCP_HOST=0.0.0.0
# MCP_PORT=8765

# Optional bearer token for client authentication (future use).
# MCP_AUTH_TOKEN=

# Soft ceiling on response size in tokens.  Higher = more evidence for
# the model, but uses more tokens.  30k is a good default for most models.
# MCP_RESPONSE_TOKEN_BUDGET=30000

# Hard timeout (seconds) for the answer tool.  Prevents runaway expansion
# from blocking the MCP connection indefinitely.
# MCP_TOOL_TIMEOUT=120

# Logging verbosity: DEBUG, INFO, WARNING
# MCP_LOG_LEVEL=INFO
